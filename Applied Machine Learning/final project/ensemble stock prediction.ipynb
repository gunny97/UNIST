{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import FinanceDataReader as fdr\n",
    "import talib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = fdr.DataReader(symbol=\"005930\", start='2011', end='2021') # 2011~2020년 삼성전자 주가데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/김건우/Desktop/UNIST/4학년 1학기/기계학습 응용/'\n",
    "df_fundamental = pd.read_excel(data_dir + 'Data_1.xlsx', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_price, df_fundamental, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>외국인지분율</th>\n",
       "      <th>60일변동성(표준편차)</th>\n",
       "      <th>60일베타</th>\n",
       "      <th>60일알파</th>\n",
       "      <th>...</th>\n",
       "      <th>P/E(MAIN, TTM)</th>\n",
       "      <th>P/B(MAIN, TTM)</th>\n",
       "      <th>P/C(TTM)</th>\n",
       "      <th>P/CE(TTM)</th>\n",
       "      <th>P/S(TTM)</th>\n",
       "      <th>P/FCF1(TTM)</th>\n",
       "      <th>P/FCF2(TTM)</th>\n",
       "      <th>EV/EBITDA(TTM)</th>\n",
       "      <th>신용융자잔고금액</th>\n",
       "      <th>60일누적대차거래잔고증감비중</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.06</td>\n",
       "      <td>0.019769</td>\n",
       "      <td>1.505725</td>\n",
       "      <td>0.163186</td>\n",
       "      <td>...</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1.67</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.69</td>\n",
       "      <td>1.58</td>\n",
       "      <td>35.68</td>\n",
       "      <td>16.82</td>\n",
       "      <td>3.66</td>\n",
       "      <td>344599630</td>\n",
       "      <td>-1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.06</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>1.504048</td>\n",
       "      <td>0.177580</td>\n",
       "      <td>...</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1.67</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.69</td>\n",
       "      <td>1.58</td>\n",
       "      <td>35.68</td>\n",
       "      <td>16.82</td>\n",
       "      <td>3.66</td>\n",
       "      <td>389517460</td>\n",
       "      <td>-1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.06</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>1.434582</td>\n",
       "      <td>0.223318</td>\n",
       "      <td>...</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1.67</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.69</td>\n",
       "      <td>1.58</td>\n",
       "      <td>35.68</td>\n",
       "      <td>16.82</td>\n",
       "      <td>3.66</td>\n",
       "      <td>389079930</td>\n",
       "      <td>-1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open  High  Low  Close  Volume  Change  외국인지분율  60일변동성(표준편차)  \\\n",
       "Date                                                                       \n",
       "2018-04-30     0     0    0  53000       0     0.0   52.06      0.019769   \n",
       "2018-05-02     0     0    0  53000       0     0.0   52.06      0.019767   \n",
       "2018-05-03     0     0    0  53000       0     0.0   52.06      0.018929   \n",
       "\n",
       "               60일베타     60일알파  ...  P/E(MAIN, TTM)  P/B(MAIN, TTM)  P/C(TTM)  \\\n",
       "Date                            ...                                             \n",
       "2018-04-30  1.505725  0.163186  ...            8.68            1.67      5.88   \n",
       "2018-05-02  1.504048  0.177580  ...            8.68            1.67      5.88   \n",
       "2018-05-03  1.434582  0.223318  ...            8.68            1.67      5.88   \n",
       "\n",
       "            P/CE(TTM)  P/S(TTM)  P/FCF1(TTM)  P/FCF2(TTM)  EV/EBITDA(TTM)  \\\n",
       "Date                                                                        \n",
       "2018-04-30       5.69      1.58        35.68        16.82            3.66   \n",
       "2018-05-02       5.69      1.58        35.68        16.82            3.66   \n",
       "2018-05-03       5.69      1.58        35.68        16.82            3.66   \n",
       "\n",
       "             신용융자잔고금액  60일누적대차거래잔고증감비중  \n",
       "Date                                    \n",
       "2018-04-30  344599630            -1.59  \n",
       "2018-05-02  389517460            -1.63  \n",
       "2018-05-03  389079930            -1.60  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 거래정지로 인한 결측치 제거\n",
    "mask = df[df['Open']==0]\n",
    "df = df.drop(mask.index)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2440.000000\n",
       "mean        0.011072\n",
       "std         0.069638\n",
       "min        -0.334308\n",
       "20%        -0.047415\n",
       "40%        -0.005069\n",
       "50%         0.013921\n",
       "60%         0.031221\n",
       "80%         0.070880\n",
       "max         0.216260\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['log_return'] = np.log(df['Close']/ df['Close'].shift(1))\n",
    "df['cum_rtn_1M']=df['log_return'].rolling(20).sum()\n",
    "df['target'] = df['cum_rtn_1M'].shift(-19) # -> 데이터 맨뒤에도 패딩 필요\n",
    "df['target'].describe(percentiles = [.2, .4, .6, .8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_label(target):\n",
    "    if target > 0.07:                      # 7% 이상\n",
    "        target =  2\n",
    "    elif target > 0.03 and target <= 0.07: # 3~7% 사이\n",
    "        target = 1\n",
    "    elif target > -0.01 and target <= 0.03: # --1~3% 사이\n",
    "        target = 0\n",
    "    elif target > -0.05 and target <= -0.01:  # -5~-1% 사이\n",
    "        target = -1\n",
    "    elif target <= -0.05: # -5% 이하\n",
    "        target = -2\n",
    "    else:\n",
    "        target = np.nan # 예외처리\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].apply(classify_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Change', 'log_return', 'cum_rtn_1M'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기술적 지표 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### price indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이동평균\n",
    "df['MA_5'] = talib.SMA(df['Close'], timeperiod=5)\n",
    "df['MA_10'] = talib.SMA(df['Close'], timeperiod=10)\n",
    "df['MA_20'] = talib.SMA(df['Close'], timeperiod=20)\n",
    "df['MA_60'] = talib.SMA(df['Close'], timeperiod=60)\n",
    "df['MA_120'] = talib.SMA(df['Close'], timeperiod=120)\n",
    "\n",
    "# 볼린저밴드\n",
    "df['BB_Up'] = talib.BBANDS(df['Close'], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)[0] # 볼린저밴드 상단\n",
    "df['BB_Down'] = talib.BBANDS(df['Close'], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)[2] #볼린저밴드 하단\n",
    "\n",
    "# 파라볼릭SAR\n",
    "df['PSAR'] = talib.SAR(df['High'], df['Low'], acceleration=0.02, maximum=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### momentum indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RSI'] = talib.RSI(df['Close'], timeperiod = 14)\n",
    "df['MACD'] = talib.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### volume indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CO'] = talib.ADOSC(df['High'], df['Low'], df['Close'], df['Volume'], fastperiod=3, slowperiod=10) # Chaikin Oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### volatility indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ATR']=talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cycle indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HT_DCPERIOD']=talib.HT_DCPERIOD(df['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기술적 지표로 인한 결측치 제거\n",
    "df.dropna(axis=0, how='any', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n_open, _high, _low, _close, _volume = df['Open'], df['High'], df['Low'], df['Close'], df['Volume']\\n\\ndf['DEMA'] = talib.DEMA(_close)\\ndf['EMA'] = talib.EMA(_close)\\ndf['HT_TRENDLINE'] = talib.HT_TRENDLINE(_close)\\ndf['KAMA'] = talib.KAMA(_close)\\ndf['MAMA'], df['FAMA'] = talib.MAMA(_close)\\ndf['SAREXT'] = talib.SAREXT(_high, _low)\\ndf['T3'] = talib.T3(_close)\\ndf['ADXR'] = talib.ADXR(_high, _low, _close)\\ndf['APO'] = talib.APO(_close)\\ndf['AROONOSC'] = talib.AROONOSC(_high, _low)\\ndf['BOP'] = talib.BOP(_open, _high, _low, _close)\\ndf['CCI'] = talib.CCI(_high, _low, _close)\\ndf['CMO'] = talib.CMO(_close)\\ndf['DX'] = talib.DX(_high, _low, _close)\\ndf['MFI'] = talib.MFI(_high, _low, _close, _volume)\\ndf['MOM'] = talib.MOM(_close)\\ndf['PPO'] = talib.PPO(_close)\\ndf['STOCH_K'], df['STOCH_D'] = talib.STOCH(_high, _low, _close)\\ndf['TRIX'] = talib.TRIX(_close)\\ndf['ULTOSC'] = talib.ULTOSC(_high, _low, _close)\\ndf['WILLR'] = talib.WILLR(_high, _low, _close)\\n\""
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "_open, _high, _low, _close, _volume = df['Open'], df['High'], df['Low'], df['Close'], df['Volume']\n",
    "\n",
    "df['DEMA'] = talib.DEMA(_close)\n",
    "df['EMA'] = talib.EMA(_close)\n",
    "df['HT_TRENDLINE'] = talib.HT_TRENDLINE(_close)\n",
    "df['KAMA'] = talib.KAMA(_close)\n",
    "df['MAMA'], df['FAMA'] = talib.MAMA(_close)\n",
    "df['SAREXT'] = talib.SAREXT(_high, _low)\n",
    "df['T3'] = talib.T3(_close)\n",
    "df['ADXR'] = talib.ADXR(_high, _low, _close)\n",
    "df['APO'] = talib.APO(_close)\n",
    "df['AROONOSC'] = talib.AROONOSC(_high, _low)\n",
    "df['BOP'] = talib.BOP(_open, _high, _low, _close)\n",
    "df['CCI'] = talib.CCI(_high, _low, _close)\n",
    "df['CMO'] = talib.CMO(_close)\n",
    "df['DX'] = talib.DX(_high, _low, _close)\n",
    "df['MFI'] = talib.MFI(_high, _low, _close, _volume)\n",
    "df['MOM'] = talib.MOM(_close)\n",
    "df['PPO'] = talib.PPO(_close)\n",
    "df['STOCH_K'], df['STOCH_D'] = talib.STOCH(_high, _low, _close)\n",
    "df['TRIX'] = talib.TRIX(_close)\n",
    "df['ULTOSC'] = talib.ULTOSC(_high, _low, _close)\n",
    "df['WILLR'] = talib.WILLR(_high, _low, _close)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    512\n",
       " 2.0    495\n",
       " 1.0    481\n",
       "-2.0    439\n",
       "-1.0    395\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['target'] = np.where(df['target']>0,1,-1)\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>외국인지분율</th>\n",
       "      <th>60일변동성(표준편차)</th>\n",
       "      <th>60일베타</th>\n",
       "      <th>60일알파</th>\n",
       "      <th>배당수익률(FY0)</th>\n",
       "      <th>...</th>\n",
       "      <th>MA_60</th>\n",
       "      <th>MA_120</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "      <th>PSAR</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>CO</th>\n",
       "      <th>ATR</th>\n",
       "      <th>HT_DCPERIOD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-06-28</th>\n",
       "      <td>16800</td>\n",
       "      <td>17060</td>\n",
       "      <td>16620</td>\n",
       "      <td>16640</td>\n",
       "      <td>239178</td>\n",
       "      <td>50.88</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>1.022287</td>\n",
       "      <td>-0.143621</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>17693.000000</td>\n",
       "      <td>18174.166667</td>\n",
       "      <td>17059.437758</td>\n",
       "      <td>16348.562242</td>\n",
       "      <td>17080.0</td>\n",
       "      <td>42.611579</td>\n",
       "      <td>-304.897785</td>\n",
       "      <td>-15341.128118</td>\n",
       "      <td>458.793021</td>\n",
       "      <td>17.738586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-29</th>\n",
       "      <td>16960</td>\n",
       "      <td>16980</td>\n",
       "      <td>16640</td>\n",
       "      <td>16660</td>\n",
       "      <td>207146</td>\n",
       "      <td>50.92</td>\n",
       "      <td>0.017934</td>\n",
       "      <td>0.993777</td>\n",
       "      <td>-0.170791</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>17657.333333</td>\n",
       "      <td>18153.333333</td>\n",
       "      <td>17044.512400</td>\n",
       "      <td>16419.487600</td>\n",
       "      <td>17060.0</td>\n",
       "      <td>42.963277</td>\n",
       "      <td>-293.176586</td>\n",
       "      <td>-95319.607354</td>\n",
       "      <td>450.307806</td>\n",
       "      <td>19.161649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-30</th>\n",
       "      <td>16660</td>\n",
       "      <td>16700</td>\n",
       "      <td>16420</td>\n",
       "      <td>16520</td>\n",
       "      <td>328339</td>\n",
       "      <td>50.95</td>\n",
       "      <td>0.017929</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>-0.202640</td>\n",
       "      <td>1.21</td>\n",
       "      <td>...</td>\n",
       "      <td>17617.666667</td>\n",
       "      <td>18131.333333</td>\n",
       "      <td>17061.262079</td>\n",
       "      <td>16362.737921</td>\n",
       "      <td>17060.0</td>\n",
       "      <td>41.066085</td>\n",
       "      <td>-291.820357</td>\n",
       "      <td>-149221.657282</td>\n",
       "      <td>438.142962</td>\n",
       "      <td>20.537234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-01</th>\n",
       "      <td>16860</td>\n",
       "      <td>17200</td>\n",
       "      <td>16760</td>\n",
       "      <td>17100</td>\n",
       "      <td>442918</td>\n",
       "      <td>50.99</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>1.035295</td>\n",
       "      <td>-0.152594</td>\n",
       "      <td>1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>17587.666667</td>\n",
       "      <td>18116.833333</td>\n",
       "      <td>17118.684684</td>\n",
       "      <td>16329.315316</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>50.765919</td>\n",
       "      <td>-241.164391</td>\n",
       "      <td>-80836.824566</td>\n",
       "      <td>455.418465</td>\n",
       "      <td>21.841461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-04</th>\n",
       "      <td>17400</td>\n",
       "      <td>17660</td>\n",
       "      <td>17380</td>\n",
       "      <td>17560</td>\n",
       "      <td>429433</td>\n",
       "      <td>51.01</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>1.051766</td>\n",
       "      <td>-0.088197</td>\n",
       "      <td>1.14</td>\n",
       "      <td>...</td>\n",
       "      <td>17572.666667</td>\n",
       "      <td>18108.166667</td>\n",
       "      <td>17668.072535</td>\n",
       "      <td>16123.927465</td>\n",
       "      <td>15926.0</td>\n",
       "      <td>56.834048</td>\n",
       "      <td>-162.033199</td>\n",
       "      <td>-6473.045312</td>\n",
       "      <td>462.888575</td>\n",
       "      <td>23.196241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close  Volume  외국인지분율  60일변동성(표준편차)  \\\n",
       "Date                                                                   \n",
       "2011-06-28  16800  17060  16620  16640  239178   50.88      0.017980   \n",
       "2011-06-29  16960  16980  16640  16660  207146   50.92      0.017934   \n",
       "2011-06-30  16660  16700  16420  16520  328339   50.95      0.017929   \n",
       "2011-07-01  16860  17200  16760  17100  442918   50.99      0.018549   \n",
       "2011-07-04  17400  17660  17380  17560  429433   51.01      0.018679   \n",
       "\n",
       "               60일베타     60일알파  배당수익률(FY0)  ...         MA_60        MA_120  \\\n",
       "Date                                        ...                               \n",
       "2011-06-28  1.022287 -0.143621        1.20  ...  17693.000000  18174.166667   \n",
       "2011-06-29  0.993777 -0.170791        1.20  ...  17657.333333  18153.333333   \n",
       "2011-06-30  0.992857 -0.202640        1.21  ...  17617.666667  18131.333333   \n",
       "2011-07-01  1.035295 -0.152594        1.17  ...  17587.666667  18116.833333   \n",
       "2011-07-04  1.051766 -0.088197        1.14  ...  17572.666667  18108.166667   \n",
       "\n",
       "                   BB_Up       BB_Down     PSAR        RSI        MACD  \\\n",
       "Date                                                                     \n",
       "2011-06-28  17059.437758  16348.562242  17080.0  42.611579 -304.897785   \n",
       "2011-06-29  17044.512400  16419.487600  17060.0  42.963277 -293.176586   \n",
       "2011-06-30  17061.262079  16362.737921  17060.0  41.066085 -291.820357   \n",
       "2011-07-01  17118.684684  16329.315316  15900.0  50.765919 -241.164391   \n",
       "2011-07-04  17668.072535  16123.927465  15926.0  56.834048 -162.033199   \n",
       "\n",
       "                       CO         ATR  HT_DCPERIOD  \n",
       "Date                                                \n",
       "2011-06-28  -15341.128118  458.793021    17.738586  \n",
       "2011-06-29  -95319.607354  450.307806    19.161649  \n",
       "2011-06-30 -149221.657282  438.142962    20.537234  \n",
       "2011-07-01  -80836.824566  455.418465    21.841461  \n",
       "2011-07-04   -6473.045312  462.888575    23.196241  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].astype(np.int64)\n",
    "y_var = df['target']\n",
    "#x_var = df.drop(['target','OPEN','HIGH','LOW','VOLUME'],axis=1)\n",
    "x_var = df.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set label ratio\n",
      " 0    0.219692\n",
      " 1    0.211692\n",
      " 2    0.211077\n",
      "-2    0.190154\n",
      "-1    0.167385\n",
      "Name: target, dtype: float64\n",
      "test set label ratio\n",
      " 0    0.222382\n",
      " 2    0.218077\n",
      " 1    0.196557\n",
      "-2    0.186514\n",
      "-1    0.176471\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.3, shuffle=False, random_state=3)\n",
    "\n",
    "train_count = y_train.count()\n",
    "test_count = y_test.count()\n",
    "\n",
    "print('train set label ratio')\n",
    "print(y_train.value_counts()/train_count)\n",
    "print('test set label ratio')\n",
    "print(y_test.value_counts()/test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1625, 33)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original data shape -> 33 features\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>외국인지분율</th>\n",
       "      <th>60일변동성(표준편차)</th>\n",
       "      <th>60일베타</th>\n",
       "      <th>60일알파</th>\n",
       "      <th>배당수익률(FY0)</th>\n",
       "      <th>...</th>\n",
       "      <th>MA_60</th>\n",
       "      <th>MA_120</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "      <th>PSAR</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>CO</th>\n",
       "      <th>ATR</th>\n",
       "      <th>HT_DCPERIOD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-06-28</th>\n",
       "      <td>16800</td>\n",
       "      <td>17060</td>\n",
       "      <td>16620</td>\n",
       "      <td>16640</td>\n",
       "      <td>239178</td>\n",
       "      <td>50.88</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>1.022287</td>\n",
       "      <td>-0.143621</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>17693.000000</td>\n",
       "      <td>18174.166667</td>\n",
       "      <td>17059.437758</td>\n",
       "      <td>16348.562242</td>\n",
       "      <td>17080.0</td>\n",
       "      <td>42.611579</td>\n",
       "      <td>-304.897785</td>\n",
       "      <td>-15341.128118</td>\n",
       "      <td>458.793021</td>\n",
       "      <td>17.738586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-29</th>\n",
       "      <td>16960</td>\n",
       "      <td>16980</td>\n",
       "      <td>16640</td>\n",
       "      <td>16660</td>\n",
       "      <td>207146</td>\n",
       "      <td>50.92</td>\n",
       "      <td>0.017934</td>\n",
       "      <td>0.993777</td>\n",
       "      <td>-0.170791</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>17657.333333</td>\n",
       "      <td>18153.333333</td>\n",
       "      <td>17044.512400</td>\n",
       "      <td>16419.487600</td>\n",
       "      <td>17060.0</td>\n",
       "      <td>42.963277</td>\n",
       "      <td>-293.176586</td>\n",
       "      <td>-95319.607354</td>\n",
       "      <td>450.307806</td>\n",
       "      <td>19.161649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-30</th>\n",
       "      <td>16660</td>\n",
       "      <td>16700</td>\n",
       "      <td>16420</td>\n",
       "      <td>16520</td>\n",
       "      <td>328339</td>\n",
       "      <td>50.95</td>\n",
       "      <td>0.017929</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>-0.202640</td>\n",
       "      <td>1.21</td>\n",
       "      <td>...</td>\n",
       "      <td>17617.666667</td>\n",
       "      <td>18131.333333</td>\n",
       "      <td>17061.262079</td>\n",
       "      <td>16362.737921</td>\n",
       "      <td>17060.0</td>\n",
       "      <td>41.066085</td>\n",
       "      <td>-291.820357</td>\n",
       "      <td>-149221.657282</td>\n",
       "      <td>438.142962</td>\n",
       "      <td>20.537234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-01</th>\n",
       "      <td>16860</td>\n",
       "      <td>17200</td>\n",
       "      <td>16760</td>\n",
       "      <td>17100</td>\n",
       "      <td>442918</td>\n",
       "      <td>50.99</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>1.035295</td>\n",
       "      <td>-0.152594</td>\n",
       "      <td>1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>17587.666667</td>\n",
       "      <td>18116.833333</td>\n",
       "      <td>17118.684684</td>\n",
       "      <td>16329.315316</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>50.765919</td>\n",
       "      <td>-241.164391</td>\n",
       "      <td>-80836.824566</td>\n",
       "      <td>455.418465</td>\n",
       "      <td>21.841461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-04</th>\n",
       "      <td>17400</td>\n",
       "      <td>17660</td>\n",
       "      <td>17380</td>\n",
       "      <td>17560</td>\n",
       "      <td>429433</td>\n",
       "      <td>51.01</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>1.051766</td>\n",
       "      <td>-0.088197</td>\n",
       "      <td>1.14</td>\n",
       "      <td>...</td>\n",
       "      <td>17572.666667</td>\n",
       "      <td>18108.166667</td>\n",
       "      <td>17668.072535</td>\n",
       "      <td>16123.927465</td>\n",
       "      <td>15926.0</td>\n",
       "      <td>56.834048</td>\n",
       "      <td>-162.033199</td>\n",
       "      <td>-6473.045312</td>\n",
       "      <td>462.888575</td>\n",
       "      <td>23.196241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close  Volume  외국인지분율  60일변동성(표준편차)  \\\n",
       "Date                                                                   \n",
       "2011-06-28  16800  17060  16620  16640  239178   50.88      0.017980   \n",
       "2011-06-29  16960  16980  16640  16660  207146   50.92      0.017934   \n",
       "2011-06-30  16660  16700  16420  16520  328339   50.95      0.017929   \n",
       "2011-07-01  16860  17200  16760  17100  442918   50.99      0.018549   \n",
       "2011-07-04  17400  17660  17380  17560  429433   51.01      0.018679   \n",
       "\n",
       "               60일베타     60일알파  배당수익률(FY0)  ...         MA_60        MA_120  \\\n",
       "Date                                        ...                               \n",
       "2011-06-28  1.022287 -0.143621        1.20  ...  17693.000000  18174.166667   \n",
       "2011-06-29  0.993777 -0.170791        1.20  ...  17657.333333  18153.333333   \n",
       "2011-06-30  0.992857 -0.202640        1.21  ...  17617.666667  18131.333333   \n",
       "2011-07-01  1.035295 -0.152594        1.17  ...  17587.666667  18116.833333   \n",
       "2011-07-04  1.051766 -0.088197        1.14  ...  17572.666667  18108.166667   \n",
       "\n",
       "                   BB_Up       BB_Down     PSAR        RSI        MACD  \\\n",
       "Date                                                                     \n",
       "2011-06-28  17059.437758  16348.562242  17080.0  42.611579 -304.897785   \n",
       "2011-06-29  17044.512400  16419.487600  17060.0  42.963277 -293.176586   \n",
       "2011-06-30  17061.262079  16362.737921  17060.0  41.066085 -291.820357   \n",
       "2011-07-01  17118.684684  16329.315316  15900.0  50.765919 -241.164391   \n",
       "2011-07-04  17668.072535  16123.927465  15926.0  56.834048 -162.033199   \n",
       "\n",
       "                       CO         ATR  HT_DCPERIOD  \n",
       "Date                                                \n",
       "2011-06-28  -15341.128118  458.793021    17.738586  \n",
       "2011-06-29  -95319.607354  450.307806    19.161649  \n",
       "2011-06-30 -149221.657282  438.142962    20.537234  \n",
       "2011-07-01  -80836.824566  455.418465    21.841461  \n",
       "2011-07-04   -6473.045312  462.888575    23.196241  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after PCA -> 4 features\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4,svd_solver='auto')\n",
    "scores = pca.fit_transform(x_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.86174783e+15, 3.53330834e+13, 3.41245754e+13, 7.49424809e+08])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.92222952e-01, 3.95613788e-03, 3.82082492e-03, 8.39108165e-08])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = pca.explained_variance_ratio_\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents = pca.fit_transform(x_var)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pc1', 'pc2', 'pc3','pc4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.3, shuffle=False, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1625"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_test,pred):\n",
    "    confusion = confusion_matrix(y_test,pred)\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    #precision = precision_score(y_test,pred)\n",
    "    #recall = recall_score(y_test,pred)\n",
    "    #f1 = f1_score(y_test,pred)\n",
    "    #roc_score = roc_auc_score(y_test,pred)\n",
    "    #print(confusion)\n",
    "    print('accuracy:{0:.4f}'.format(accuracy))\n",
    "    #print('accuracy:{0:.4f},precision:{1:.4f},recall:{2:.4f},F1:{3:.4f},ROC AUC score:{4:.4f}'.format(accuracy,precision,recall,f1,roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(100,500,100)\n",
    "params ={\n",
    "    'objective': ['multi:softmax'],\n",
    "    'num_class': [class_num],\n",
    "    'n_estimators':n_estimators,\n",
    "    'max_depth':[4,6],\n",
    "    'gamma' : [0,0.1],\n",
    "    'min_child_weight': [1,3,5],\n",
    "    'subsample' : [0.6,0.8],\n",
    "    'colsample_bytree' : [0.6,0.8],\n",
    "    'learning_rate' : [0.05,0.1],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cv = TimeSeriesSplit(n_splits=5).split(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:59:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001B020AF34A0>,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     miss...\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.6, 0.8], 'gamma': [0, 0.1],\n",
       "                         'learning_rate': [0.05, 0.1], 'max_depth': [4, 6],\n",
       "                         'min_child_weight': [1, 3, 5],\n",
       "                         'n_estimators': range(100, 500, 100), 'num_class': [5],\n",
       "                         'objective': ['multi:softmax'],\n",
       "                         'subsample': [0.6, 0.8]})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = GridSearchCV(XGBClassifier(),params,cv=my_cv,n_jobs=-1)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters\n",
      " {'colsample_bytree': 0.6, 'gamma': 0.1, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 100, 'num_class': 5, 'objective': 'multi:softmax', 'subsample': 0.6}\n",
      "best prediction:0.2630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('best parameters\\n',xgb.best_params_)\n",
    "print('best prediction:{0:.4f}\\n'.format(xgb.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.1951\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb.predict(X_test)\n",
    "accuracy_con = accuracy_score(y_test,xgb_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "#get_confusion_matrix(y_test,xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(100,500,100)\n",
    "params ={\n",
    "    #'objective ': ['multiclass'],\n",
    "    'num_class': [class_num],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'n_estimators': n_estimators,\n",
    "    'num_boost_round' : [100,200,300],\n",
    "    'metric' : ['multi_logloss'],\n",
    "    'learning_rate ':[0.05,0.1],\n",
    "    'num_leaves':[20],\n",
    "    'max_depth':[4,6,8],\n",
    "    'sub_feature':[0.4,0.6],\n",
    "    'min_data_in_leaf' : range(30,60,10),\n",
    "    'feature_fraction': [0.4,0.6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cv = TimeSeriesSplit(n_splits=5).split(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "차원축소하지 않은 원본데이터 사용할때, column명 바꿔주기\n",
    "'''\n",
    "X_train.columns = [i for i in range(len(X_train.columns))]\n",
    "X_test.columns = [i for i in range(len(X_test.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001B029E41CF0>,\n",
       "             estimator=LGBMClassifier(objective='multiclass'), n_jobs=-1,\n",
       "             param_grid={'boosting_type': ['gbdt'],\n",
       "                         'feature_fraction': [0.4, 0.6],\n",
       "                         'learning_rate ': [0.05, 0.1], 'max_depth': [4, 6, 8],\n",
       "                         'metric': ['multi_logloss'],\n",
       "                         'min_data_in_leaf': range(30, 60, 10),\n",
       "                         'n_estimators': range(100, 500, 100),\n",
       "                         'num_boost_round': [100, 200, 300], 'num_class': [5],\n",
       "                         'num_leaves': [20], 'sub_feature': [0.4, 0.6]})"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb = GridSearchCV(LGBMClassifier(objective='multiclass'),params,cv=my_cv,n_jobs=-1)\n",
    "lgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters\n",
      " {'boosting_type': 'gbdt', 'feature_fraction': 0.6, 'learning_rate ': 0.05, 'max_depth': 8, 'metric': 'multi_logloss', 'min_data_in_leaf': 30, 'n_estimators': 100, 'num_boost_round': 100, 'num_class': 5, 'num_leaves': 20, 'sub_feature': 0.4}\n",
      "best prediction:0.2504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('best parameters\\n',lgb.best_params_)\n",
    "print('best prediction:{0:.4f}\\n'.format(lgb.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.2052\n",
      "accuracy:0.2052\n"
     ]
    }
   ],
   "source": [
    "lgb_pred = lgb.predict(X_test)\n",
    "accuracy_con = accuracy_score(y_test,lgb_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,lgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(100,500,100)\n",
    "rf_param = {\n",
    "    'n_estimators':n_estimators,\n",
    "    'max_depth':[4,6,8],\n",
    "    'min_samples_leaf':[3,4,5,6],\n",
    "    'min_samples_split':[2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cv = TimeSeriesSplit(n_splits=5).split(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001B020AB4900>,\n",
       "             estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6, 8],\n",
       "                         'min_samples_leaf': [3, 4, 5, 6],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': range(100, 500, 100)},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = GridSearchCV(RandomForestClassifier(), param_grid = rf_param, scoring='accuracy', cv=my_cv,n_jobs=-1, verbose=2)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters\n",
      " {'max_depth': 4, 'min_samples_leaf': 5, 'min_samples_split': 4, 'n_estimators': 100}\n",
      "best prediction:0.2674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('best parameters\\n',rf.best_params_)\n",
    "print('best prediction:{0:.4f}\\n'.format(rf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.2339\n",
      "accuracy:0.2339\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf.predict(X_test)\n",
    "accuracy_con = accuracy_score(y_test,rf_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cv = TimeSeriesSplit(n_splits=5).split(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(100,500,100)\n",
    "\n",
    "ext_param = {\n",
    "    'n_estimators':n_estimators,\n",
    "    'max_depth':[4,6,8],\n",
    "    'min_samples_leaf':[3,4,5,6],\n",
    "    'min_samples_split':[2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001B029C73F20>,\n",
       "             estimator=ExtraTreesClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6, 8],\n",
       "                         'min_samples_leaf': [3, 4, 5, 6],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': range(100, 500, 100)},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext = GridSearchCV(ExtraTreesClassifier(), param_grid = ext_param, scoring='accuracy', cv=my_cv,n_jobs=-1, verbose=2)\n",
    "ext.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters\n",
      " {'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "best prediction:0.2711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('best parameters\\n',ext.best_params_)\n",
    "print('best prediction:{0:.4f}\\n'.format(ext.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.2009\n",
      "accuracy:0.2009\n"
     ]
    }
   ],
   "source": [
    "ext_pred = ext.predict(X_test)\n",
    "accuracy_con = accuracy_score(y_test,ext_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,ext_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[23:33:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[23:33:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[23:33:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[23:33:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[23:33:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.1722\n",
      "accuracy:0.1722\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[xgb_clf ,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  2,  2,  1,  2,  2,  2,\n",
       "        1,  2,  2,  1,  1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  2,  2,  2,  1,  1,  2,  2,  2,  1,  1,  1,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  1,  1,  2,  2, -2,  2,  1,\n",
       "        1,  1,  0,  0,  0,  1,  2,  2,  2,  2,  0,  2,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,\n",
       "        1,  0, -1,  1,  1,  1,  2,  1,  1,  1, -1, -1, -1,  1,  1, -1, -1,\n",
       "        2,  2,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,\n",
       "        1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  1,  2,  2,  2,  2,  2,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  2,  1,  1, -2,\n",
       "       -2,  1,  1,  1,  1,  1,  1,  1,  1, -2,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  2,  0,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "        2,  1,  1,  1,  1,  1,  1,  1,  1,  2, -2, -2, -2,  0, -2, -2, -2,\n",
       "        0, -2, -2, -2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,\n",
       "        2,  2,  2,  0,  2,  2,  2,  2,  2,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "        0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  1,  1,  1, -2, -2, -2, -2,\n",
       "       -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2,\n",
       "       -2, -2,  0,  2,  2,  2,  0,  0,  0,  0, -1,  2,  2,  0,  2, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,  1, -1, -1, -1, -1,\n",
       "        2, -1, -1,  2,  2,  2, -1, -1, -1, -1,  0,  0,  1,  0, -1,  0, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  2, -1, -1,  2, -1, -1, -2,  2,  0,  0, -1,\n",
       "       -1, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1, -1,\n",
       "       -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0, -1, -1,  2,  2,  2,\n",
       "        2, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2, -1,  1,  0,  1,  1,  1,  1,  2,  1,  2,  2, -1, -1, -2,\n",
       "       -1, -1, -1,  2, -1,  2,  1,  2,  1,  2,  2,  2,  2,  2,  2, -1,  1,\n",
       "        2,  1,  2,  2, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  2,  2,\n",
       "        2, -1, -1,  2,  1,  2,  2,  2, -1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1, -1,  1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1,  2,  1,  1, -1,\n",
       "        1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,  2, -1,  2,  2,  2,\n",
       "        2, -1, -1,  2,  1, -1, -1, -1, -1,  1, -1,  2,  2,  2, -1, -1, -1,\n",
       "       -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1,  2,  2,\n",
       "        2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2,  2, -1,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[23:33:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[23:33:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[23:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[23:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "[23:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2152\n",
      "accuracy:0.2152\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[xgb_clf, rf_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  1,  2,  2,  2,  1,  1,  1,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  1,  2,  2,  2,  1,  2,  1,  1,  1,  1,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1,\n",
       "        1, -2, -2,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1, -2, -2,  2,  1,\n",
       "        1,  1, -1,  1,  1,  1,  2,  2,  2,  2,  0,  0, -1,  1,  1,  0,  0,\n",
       "        2,  2,  2,  2,  2,  2, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2, -2,\n",
       "       -2, -2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1, -2,  1,  1,  1,  1,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,  1,  2,  2,  2,  2,  2,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, -1,  0, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2, -2, -2,  1, -2,  1, -2, -2,  1,  2,  2,  2,  2,  1,  2,  2,  2,\n",
       "        1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1,  0, -1, -1, -1, -1, -1,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "        0,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1,  2,  2, -2, -2, -2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  0, -1, -1, -1,  0,  0,  0,  0,  0, -1, -1,  0, -1, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  2,  0,  0,  2,  0,  0,  2,  2,  0,  0,  0,\n",
       "       -1, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1, -1,\n",
       "       -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1, -1, -1,  2,  2,  2,\n",
       "        2, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2, -1,  1,  1,  1,  1,  1,  1,  1, -2, -2, -2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2, -2, -2,  1,  1, -2,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  2,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
       "        1, -1, -1,  1,  2,  2,  1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,\n",
       "        1, -1,  1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1,  1,  1,  1, -1,\n",
       "        1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,  2, -1,  2,  2,  2,\n",
       "        2, -1, -1,  2,  1, -1, -1, -1, -1,  1, -1,  2,  2,  2, -1, -1, -1,\n",
       "       -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1,  2,  2,\n",
       "        2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2,  2, -1,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[00:07:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[00:07:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[00:07:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[00:07:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "[00:07:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2123\n",
      "accuracy:0.2123\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=0,\n",
    "                        boosting_type = lgb.best_params_['boosting_type'],\n",
    "                        feature_fraction = lgb.best_params_['feature_fraction'],\n",
    "                        min_data_in_leaf = lgb.best_params_['min_data_in_leaf'],\n",
    "                        sub_feature = lgb.best_params_['sub_feature'],\n",
    "                        max_depth = lgb.best_params_['max_depth'],\n",
    "                        metric = lgb.best_params_['metric'],\n",
    "                        n_estimators = lgb.best_params_['n_estimators'],\n",
    "                        num_boost_round = lgb.best_params_['num_boost_round'],\n",
    "                        num_leaves = lgb.best_params_['num_leaves'],\n",
    "                        )\n",
    "\n",
    "\n",
    "ensemble_models =[xgb_clf, lgb_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,\n",
       "        0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,\n",
       "        2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  1,  2,  1,  1,\n",
       "        1,  2,  1,  1,  0,  2,  1,  1,  2,  2,  0,  0,  0,  0,  0,  0,  1,\n",
       "        0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  1,\n",
       "        2,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  1,  0,  2,  2,  2,  2,  1,  0,  0,  0,  1,  1,  0,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  0,\n",
       "        0,  0,  0,  2,  2,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  0,  0,  0,  0,  1,  1,\n",
       "        1,  2,  2,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  0,  1,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,\n",
       "        0,  0,  1,  1,  1,  0,  0,  0,  0, -1,  0,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  2,  1,  2,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  2,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  1,  1,\n",
       "       -1,  1,  1,  0,  0,  0,  0,  0,  0,  2,  2, -1, -1, -1, -1, -1, -1,\n",
       "        1, -1,  2,  2,  1,  2,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  0,  2,  2,  2,  2,  2,  0,  2,  2,  2,  2,  2,  2,  2,\n",
       "       -1, -1, -1, -1, -1,  0,  2,  2,  2,  2,  2,  2,  1, -1,  2, -1,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  0,  2,  1,  1,  0,  0,  0,  0,  0,  1,  2,  0,  2,  1, -1,\n",
       "        0,  0, -1,  0,  0, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        1,  0,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, -1, -1,  2,  1,  0,  2,  0, -1,  2,  2, -1,  0,  0,\n",
       "        0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  0, -1,  0, -2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  1, -1,  0,\n",
       "        0,  0,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        1,  1, -1,  1,  2,  2,  2,  2, -1, -1, -1,  0,  1,  0,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  0,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  0,  1,  1,  1,  1,  1,  1,  0,  1,  0,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "[00:07:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2425\n",
      "accuracy:0.2425\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=0,\n",
    "                        boosting_type = lgb.best_params_['boosting_type'],\n",
    "                        feature_fraction = lgb.best_params_['feature_fraction'],\n",
    "                        min_data_in_leaf = lgb.best_params_['min_data_in_leaf'],\n",
    "                        sub_feature = lgb.best_params_['sub_feature'],\n",
    "                        max_depth = lgb.best_params_['max_depth'],\n",
    "                        metric = lgb.best_params_['metric'],\n",
    "                        n_estimators = lgb.best_params_['n_estimators'],\n",
    "                        num_boost_round = lgb.best_params_['num_boost_round'],\n",
    "                        num_leaves = lgb.best_params_['num_leaves'],\n",
    "                        )\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[lgb_clf, rf_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  2,  2,  0,  0,  0,  0,  2,\n",
       "        2,  2,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  0,  2,  2,\n",
       "        0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  2,  0,  2,  2,\n",
       "        2,  2,  2,  1,  0,  0,  2,  2,  2,  2,  2,  2,  0,  1,  1,  1,  0,\n",
       "        2,  1,  2,  2,  2,  2,  2,  2,  0,  0,  2,  0,  0,  0,  0,  0,  2,\n",
       "        0,  2,  2,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2,  1,  1,  1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  0,  2,  2,  2,  2, -1, -1,  0,  2, -1,  0,  0,  2,  2, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -2, -1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2, -1,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1,  0,  0,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1,  1,  1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  0,  0,  0,  0, -2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  0,  2,  2,  2,  2,  2,  2,  2,  0,  0, -1,  0, -1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  1,  1,  2,  2,  2,  2, -1, -1, -1,  2,  2,\n",
       "        2,  2,  2,  0, -1,  0,  2,  2,  2,  2,  2,  2,  2, -1,  2,  2,  2,\n",
       "        2,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "       -2, -2, -2, -2, -2,  0,  0,  0,  0, -2, -2,  2,  2,  1, -2, -1,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -2,\n",
       "       -1,  0, -2,  0,  0, -2, -2, -2, -1, -1, -1, -1, -1,  0, -1, -1, -1,\n",
       "       -1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -2, -2,  2,  2,  2,  2,  0,  2,  2,  2, -2, -1, -1,\n",
       "        1,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,\n",
       "        2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  1,  1,  0,  0,  0, -1,  0,  2,  0,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  2,  1,  1,\n",
       "        1,  1,  2,  2, -1,  2,  2,  2,  2,  2,  2,  2,  2, -1, -1,  2,  1,\n",
       "        1,  1,  1,  1,  0,  2,  1,  1,  1, -1, -1,  0, -1,  0,  2,  0,  0,\n",
       "        0,  0,  2,  2, -1, -1,  0,  0,  0,  0,  0,  0,  1,  2,  1,  0,  0,\n",
       "        0,  2,  0,  0,  0,  2,  0,  2,  0,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  2,  1,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  1,  1,  1,  2,  1,\n",
       "        1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB + ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "model  1:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[00:08:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2310\n",
      "accuracy:0.2310\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[lgb_clf,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  2,  2,\n",
       "        1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  2,  1,  1,  1,  1,  1,  1,  1,  2,  2,  1,  2,  1,  1,  1,  0,\n",
       "        2,  0,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  1,  2,  1,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  1,  2,  1,  2,  2, -2, -2, -2,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1, -2, -2, -2, -2,  1,  2,  1,  2,  1,  1,  1,  1,  1,  1,  2,\n",
       "        2,  2,  2, -2,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  2,  1,  1,  1,  1,  1,\n",
       "        1,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  1,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0, -2, -2, -1, -1, -1, -1, -1,  0,  0,  0, -2, -2,  1,  1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2, -2,  2,  2,  1,  2,  1,  2,  2, -2,  0, -2,  0, -2,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  1,  1,  0,  1,  1,  1,\n",
       "        1, -2,  2,  2,  1,  2,  1,  1,  1,  2,  2,  2,  2,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  0,  0,  0,  0,  2,  2,  2,  1,  1,  2, -2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  0,  0, -1, -1, -1, -1, -1, -1, -1,  1, -2, -2, -2, -1,  2,\n",
       "       -2,  0,  2,  0,  0,  2,  2,  2, -2,  0,  1,  1,  1, -1,  0,  0,  0,\n",
       "        0,  0,  1,  1,  0,  0,  0,  0,  1,  0, -1, -1,  1, -1,  1, -2,  0,\n",
       "        1,  1,  1,  1,  2,  2,  2,  1,  0,  0, -1,  2,  2,  2,  2, -2,  1,\n",
       "        0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,\n",
       "        1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  2,\n",
       "        1,  1,  1,  2,  1,  2,  2,  1,  2,  1,  1,  1,  1,  1,  1,  1,  2,\n",
       "        1,  2,  2,  1, -1,  1,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  2,  2,  2,  2,  2, -1, -1,  1,  0,  1,  1,  1,  1,\n",
       "        1,  0,  1,  1, -1, -1,  1,  1,  1,  1,  0,  0,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  0,  1,  0,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  0,  0,  0,  1,  0,\n",
       "        0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF + ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "model  1:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[00:08:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2310\n",
      "accuracy:0.2310\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[rf_clf,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  2, -2, -2,  2, -2, -2,  1,\n",
       "        0,  1,  1,  0,  0,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  0,  2,  2,  2,  2,  0,  2,  2,  0, -2,  1,  1,  1,  0,\n",
       "        2,  2,  2,  2,  2,  2, -2,  1, -2, -2,  2, -2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2, -2, -2, -2, -2, -2, -2,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2,  1,  1,  1,  2,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  2,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        1,  2,  1,  2,  2,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1, -2,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  1,  1,  1,  1,  1,  2,  1,  1,  1, -2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, -2,  0,  0,  0,  0,  0,  0, -2,  1,  1, -2, -2, -2,  1,\n",
       "        1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,\n",
       "        1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  1,  0, -1,  0, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0, -1,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2, -2,  2, -2,  1,  1, -2,  1,  0,\n",
       "        1,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -2, -2,  2,  0,  0,  0,  0,  0,  2, -1,  2,  2,  2,  2,\n",
       "        2,  0,  0,  0,  0,  2,  2,  2,  2,  2,  0,  0,  0,  0,  0,  2,  2,\n",
       "        2,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + LGB + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[00:08:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[00:08:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[00:08:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[00:08:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "[00:08:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2425\n",
      "accuracy:0.2425\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=0,\n",
    "                        boosting_type = lgb.best_params_['boosting_type'],\n",
    "                        feature_fraction = lgb.best_params_['feature_fraction'],\n",
    "                        min_data_in_leaf = lgb.best_params_['min_data_in_leaf'],\n",
    "                        sub_feature = lgb.best_params_['sub_feature'],\n",
    "                        max_depth = lgb.best_params_['max_depth'],\n",
    "                        metric = lgb.best_params_['metric'],\n",
    "                        n_estimators = lgb.best_params_['n_estimators'],\n",
    "                        num_boost_round = lgb.best_params_['num_boost_round'],\n",
    "                        num_leaves = lgb.best_params_['num_leaves'],\n",
    "                        )\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[xgb_clf, lgb_clf, rf_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  2,  2,  2,  2,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  1,  2,  0,  0,  0,  1,\n",
       "        2,  1,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  0,  2,\n",
       "        2,  2,  2,  0,  0,  0,  2,  1,  0,  1,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  0,  2,  1,  2,  2, -2, -2,  2,  1,  0,  0,  2,  1,  1,  0,\n",
       "       -2, -2, -2, -2, -2, -2, -2,  1, -2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -2,\n",
       "       -2, -2,  2,  2,  2,  1,  1,  1,  2,  1,  1,  2, -2,  2,  2,  1,  1,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  1, -2,  1,  2,  2,  2,  2,  2,\n",
       "        1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,\n",
       "        0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, -1,  1, -1, -1,  2,  2,  2,  2, -1, -1, -1,  2,  2,\n",
       "        2,  1,  1,  2, -2,  2,  2,  2,  2,  2,  2,  2, -2,  2, -2, -2, -2,\n",
       "        1, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1, -1, -1, -1,\n",
       "       -1, -1, -1,  0, -1, -1, -1, -1, -1,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "        0,  0,  0,  0,  0,  0, -1, -1, -1,  1,  1,  2,  2,  0, -2, -2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  0, -1, -1, -1,  0,  0,  0,  0,  0, -1, -2,  0, -2, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  1, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  2,  1,  2,  2,  0,  2,  2,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,\n",
       "        1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2, -2,  1,  1,  2,  2,  2,\n",
       "        2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1, -2, -1,  2,  0,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  1, -1,  0,\n",
       "        0,  0,  2,  2,  0,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,\n",
       "        1, -1, -1,  1,  2,  2,  1,  1, -1,  0,  0,  2, -1,  2,  1,  2,  2,\n",
       "        2,  0,  1,  0,  0, -1,  2,  2,  0,  2,  0,  0, -1,  2,  1,  2,  0,\n",
       "        2,  0,  0,  0,  0,  1,  0,  1,  0,  1,  1,  1,  2,  1,  2,  2,  2,\n",
       "        2,  0,  1,  1,  1,  1,  1,  1,  0,  1,  0,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  0,  2,  2,\n",
       "        2,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  2,  1,  0,  1,  2,  1,\n",
       "        1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + LGB + ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[00:08:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[00:08:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[00:08:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[00:08:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "model  2:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[00:08:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2138\n",
      "accuracy:0.2138\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=0,\n",
    "                        boosting_type = lgb.best_params_['boosting_type'],\n",
    "                        feature_fraction = lgb.best_params_['feature_fraction'],\n",
    "                        min_data_in_leaf = lgb.best_params_['min_data_in_leaf'],\n",
    "                        sub_feature = lgb.best_params_['sub_feature'],\n",
    "                        max_depth = lgb.best_params_['max_depth'],\n",
    "                        metric = lgb.best_params_['metric'],\n",
    "                        n_estimators = lgb.best_params_['n_estimators'],\n",
    "                        num_boost_round = lgb.best_params_['num_boost_round'],\n",
    "                        num_leaves = lgb.best_params_['num_leaves'],\n",
    "                        )\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[xgb_clf, lgb_clf,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1, -1,  2,  1,  2,  2,  2,\n",
       "        1,  2, -1,  1,  1, -1, -1,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,\n",
       "        1,  2,  2,  2,  2,  1,  1,  2,  2,  2,  1,  1,  1,  2,  1,  2,  2,\n",
       "        2,  2,  2,  2,  1,  1,  2,  2,  1,  2,  2,  1,  2,  2,  1,  2,  1,\n",
       "        2,  1, -1, -1, -1,  2,  2,  2,  2,  2, -1,  2,  1,  1,  1,  1,  2,\n",
       "        1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  1,  2,  1,  1,  2,  2,  2,  2, -1, -1,\n",
       "        2,  0,  0,  2,  1,  2,  2,  1,  1,  1,  1,  0,  0,  1,  1,  1,  0,\n",
       "        1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  1,  1,  1,  2,  1,  1,  2,  1,  1,  1,  1,  1,\n",
       "        2,  1,  1,  2,  2,  2,  2,  2,  2,  1,  2,  1,  2,  2,  2,  2,  2,\n",
       "        1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,\n",
       "        0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  1,  1,  2,  1,  1, -2,\n",
       "       -2,  1,  1,  1,  1,  2,  1,  2,  1, -2, -2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  2,  2,  2,  2,  2,  2,  1,  1,  1,  2,  2,\n",
       "        2,  1,  1,  1,  1,  1,  2,  2,  2,  2, -2, -1, -1,  0, -1, -1, -1,\n",
       "       -1, -1, -2, -2,  2,  1,  2,  2,  2,  1,  1,  1,  1,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "        0,  0,  0,  2,  0,  0,  0,  0,  0,  2,  1,  1,  2, -1, -2, -1, -2,\n",
       "       -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2,\n",
       "       -2, -2,  0,  0,  1,  1,  0,  0,  0,  0, -1,  2, -2,  0, -2, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,\n",
       "        1,  0,  1,  2,  1,  1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, -1, -1,  2,  1,  0,  2, -1, -1, -2,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,\n",
       "        1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2, -1,  1,  1,  2,  2,  2,\n",
       "        2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  2, -1,  1, -2,\n",
       "        1,  1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1,  1,  2,\n",
       "        2,  2,  2,  1, -1,  1,  1,  1,  1,  1,  0, -1, -1,  1,  1,  2, -1,\n",
       "       -1, -1,  1,  1,  2,  2,  2,  2, -1,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  0,  1,  0, -1, -1,  1,  1,  0,  1,  0,  0,  1,  1,  1,  1,  0,\n",
       "        1,  0,  0,  0,  0,  1,  0,  1,  0,  1,  1,  1,  2,  1,  2,  2,  2,\n",
       "        2,  0,  1,  1,  1,  1,  1,  1,  0,  1,  0,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  0,  2,  2,\n",
       "        2,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  2,  1,  0,  1,  2,  1,\n",
       "        1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + RF +ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[23:34:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[23:34:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[23:34:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[23:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "model  2:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[23:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2052\n",
      "accuracy:0.2052\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[xgb_clf, rf_clf,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  0,  1,  1,  0,  1,  1,  0,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  2,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2, -2,  2,  2,  2,  0,  0,  2,  2,  2,  0,  0,  0,  2,  2,  2,  2,\n",
       "        2,  2,  2,  0,  0,  2,  2,  2,  0,  2,  0,  1,  1,  1, -1,  1,  1,\n",
       "        0,  0,  2,  2,  2,  0,  1,  1,  1,  1,  2,  1,  0,  0,  0,  2,  2,\n",
       "        2,  2,  2,  0,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  1,  0,\n",
       "        0,  2,  2,  1,  1,  1,  0,  1,  1,  1,  1,  0,  0,  2,  2,  2,  2,\n",
       "        0,  1, -1,  0,  0,  0,  2,  1,  1,  2,  2,  2, -1,  0,  0,  2,  2,\n",
       "        2,  2,  1,  1,  1,  1, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,\n",
       "        2,  1,  2,  1,  1,  2,  2,  2,  2,  0,  2,  0,  2,  2,  2,  2,  2,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  1,  1,  2,  1,  1,  2,\n",
       "        2,  1,  2,  2,  2,  2,  2,  1,  1, -2,  1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  1,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  0,  2,  0,  2,  2,  0,  2,  2,  2, -2,  1, -2, -2, -2,\n",
       "        1, -2,  2, -2,  2,  2,  2,  2,  1,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  0,  1,  1,  1,  1,  1,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "        0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  2,  2, -2, -2, -2, -2,\n",
       "       -2,  2,  2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2,\n",
       "       -2,  1,  0,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  1, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -1, -2, -2, -2, -2,\n",
       "       -1, -2, -2, -1, -1, -1, -2, -2, -2, -2,  0,  0,  1,  0, -2,  0, -2,\n",
       "       -2, -2, -2, -2, -2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0, -2,\n",
       "       -1, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1, -1,\n",
       "       -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1, -1,  2,  2,  2,\n",
       "        2, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  2,\n",
       "        2,  2,  2, -1,  0,  2,  0,  0,  1,  1,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  0,  1,  2,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  2,  0, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
       "        1, -1, -1,  1, -2,  2,  0,  0, -1,  1,  1,  0, -1,  0,  0,  0,  0,\n",
       "        0, -1,  1, -1, -1, -1,  0,  0, -1,  0, -1, -1, -1,  0,  1,  0, -1,\n",
       "        0, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1,  2, -1,  2,  2,  2,\n",
       "        2, -1, -1,  2,  0, -1, -1, -1, -1,  0, -1,  2,  2,  2, -1, -1, -1,\n",
       "       -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1,  2,  2,\n",
       "        2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2,  2, -1,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB + RF + ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "model  2:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[00:09:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2310\n",
      "accuracy:0.2310\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=0,\n",
    "                        boosting_type = lgb.best_params_['boosting_type'],\n",
    "                        feature_fraction = lgb.best_params_['feature_fraction'],\n",
    "                        min_data_in_leaf = lgb.best_params_['min_data_in_leaf'],\n",
    "                        sub_feature = lgb.best_params_['sub_feature'],\n",
    "                        max_depth = lgb.best_params_['max_depth'],\n",
    "                        metric = lgb.best_params_['metric'],\n",
    "                        n_estimators = lgb.best_params_['n_estimators'],\n",
    "                        num_boost_round = lgb.best_params_['num_boost_round'],\n",
    "                        num_leaves = lgb.best_params_['num_leaves'],\n",
    "                        )\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[lgb_clf, rf_clf,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  2, -2,  2,  0,  2,  2,  2,\n",
       "        1,  2,  1,  0,  0,  1,  2,  2,  2,  2,  2,  2,  2,  2,  0,  2,  2,\n",
       "        0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  2,  0,  2,  2,\n",
       "        2,  2,  2,  0,  0,  0,  2,  2,  1,  2,  2,  1,  2,  2,  2,  2,  0,\n",
       "        2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  0,  2,\n",
       "        0,  2,  2,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -1,  2,\n",
       "        2,  2,  2, -1, -1, -1,  2,  1,  2,  1,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  1,  0,  2,  2,  2,  2, -1, -1, -2,  2,  2,  0,  0,  2,  2,  2,\n",
       "        2,  2, -1, -1, -1, -1, -1,  2, -1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2, -1,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  2,  2,\n",
       "        2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0, -2, -2, -1, -1, -1, -1, -1,  0,  0,  0, -2, -2,  1,  1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2, -2,  1,  1,  2,  1,  2,  2,  1, -2,  0, -2,  0, -2,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2, -2, -2, -2,  2,  2,\n",
       "        2,  2,  2,  0,  2,  0,  2,  2,  2,  2,  2,  2,  2, -2,  2,  2,  2,\n",
       "        2, -2,  2,  1,  2,  1,  2,  2,  2,  1,  1,  1, -2,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2, -2,  2,  0,  0,  0,  0,  2, -2,  1,  2,  1, -2, -2,  1,\n",
       "        1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,\n",
       "        1,  1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  1, -2, -2, -2, -1,  2,\n",
       "       -2,  0,  2,  0,  0,  2,  2,  2, -2,  0, -1, -1, -1, -1,  0,  0,  0,\n",
       "        0,  0, -1, -1,  0,  0,  0,  0, -1,  0, -1, -1, -1, -1, -1, -2,  0,\n",
       "       -1, -1, -1, -1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -1,\n",
       "       -2,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -2,\n",
       "        2,  2,  2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2, -2, -2, -2,  0,  0,  0,  2, -1,  2, -1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2,  2,  2,\n",
       "        2,  2,  2,  1, -1,  1,  1,  1,  1,  1,  1,  2,  2, -1, -1,  2,  1,\n",
       "        1,  1,  2,  1,  2,  2,  2,  2,  2, -1, -1,  0,  0,  0,  2,  0,  0,\n",
       "        0,  0,  1,  1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
       "        0,  1,  0,  0,  0,  2,  0,  2,  0,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2, -2,  2, -2,  2,  2,  2,  2, -2, -2, -2, -2, -2, -2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -2, -2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2, -2,  2,  2,  2,  2,  2, -2, -2, -2,  2, -2,\n",
       "       -2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB + LGB + RF + ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [5]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [4]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[00:09:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.35135135]\n",
      "[00:09:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.15517241]\n",
      "[00:09:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [0.25123153]\n",
      "[00:09:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.16256158]\n",
      "    ----\n",
      "    MEAN:     [0.23007922] + [0.07956760]\n",
      "    FULL:     [0.23015385]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  0:  [0.33660934]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  1:  [0.17733990]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  2:  [0.25123153]\n",
      "[LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.4. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, sub_feature=0.4 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] num_iterations is set=100, num_boost_round=100 will be ignored. Current value: num_iterations=100\n",
      "    fold  3:  [0.18226601]\n",
      "    ----\n",
      "    MEAN:     [0.23686169] + [0.06457480]\n",
      "    FULL:     [0.23692308]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.34152334]\n",
      "    fold  1:  [0.13300493]\n",
      "    fold  2:  [0.24137931]\n",
      "    fold  3:  [0.19211823]\n",
      "    ----\n",
      "    MEAN:     [0.22700645] + [0.07644307]\n",
      "    FULL:     [0.22707692]\n",
      "\n",
      "model  3:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.31449631]\n",
      "    fold  1:  [0.12315271]\n",
      "    fold  2:  [0.22413793]\n",
      "    fold  3:  [0.13054187]\n",
      "    ----\n",
      "    MEAN:     [0.19808221] + [0.07811413]\n",
      "    FULL:     [0.19815385]\n",
      "\n",
      "[00:09:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy:0.2410\n",
      "accuracy:0.2410\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=0,\n",
    "                        colsample_bytree = xgb.best_params_['colsample_bytree'],\n",
    "                        min_child_weight = xgb.best_params_['min_child_weight'],\n",
    "                        subsample = xgb.best_params_['subsample'],\n",
    "                        gamma = xgb.best_params_['gamma'],\n",
    "                        learning_rate = xgb.best_params_['learning_rate'],\n",
    "                         max_depth = xgb.best_params_['max_depth'],\n",
    "                         n_estimators = xgb.best_params_['n_estimators'],\n",
    "                         num_class = xgb.best_params_['num_class'],\n",
    "                         objective = xgb.best_params_['objective']\n",
    "                        )\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=0,\n",
    "                        boosting_type = lgb.best_params_['boosting_type'],\n",
    "                        feature_fraction = lgb.best_params_['feature_fraction'],\n",
    "                        min_data_in_leaf = lgb.best_params_['min_data_in_leaf'],\n",
    "                        sub_feature = lgb.best_params_['sub_feature'],\n",
    "                        max_depth = lgb.best_params_['max_depth'],\n",
    "                        metric = lgb.best_params_['metric'],\n",
    "                        n_estimators = lgb.best_params_['n_estimators'],\n",
    "                        num_boost_round = lgb.best_params_['num_boost_round'],\n",
    "                        num_leaves = lgb.best_params_['num_leaves'],\n",
    "                        )\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                              max_depth=rf.best_params_['max_depth'],\n",
    "                              min_samples_leaf=rf.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=rf.best_params_['min_samples_split'],\n",
    "                              n_estimators=rf.best_params_['n_estimators'])\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(random_state=0,\n",
    "                              max_depth=ext.best_params_['max_depth'],\n",
    "                              min_samples_leaf=ext.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=ext.best_params_['min_samples_split'],\n",
    "                              n_estimators=ext.best_params_['n_estimators'])\n",
    "\n",
    "ensemble_models =[xgb_clf, lgb_clf, rf_clf,ext_clf]\n",
    "s_train, s_test = stacking(ensemble_models, X_train, y_train, X_test, \n",
    "                           mode = 'oof_pred_bag', regression=False, metric=accuracy_score, n_folds=4,\n",
    "                          shuffle=False, random_state=123, verbose=2)\n",
    "\n",
    "final_model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.05, \n",
    "                            n_estimators = 200, max_depth = 4)\n",
    "\n",
    "final_model = final_model.fit(s_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(s_test)\n",
    "accuracy_con = accuracy_score(y_test,y_pred)\n",
    "print('accuracy:{0:.4f}'.format(accuracy_con))\n",
    "get_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  2,  0,  2,  2,  2,\n",
       "        1,  2, -1,  1,  1, -1,  2,  1,  2,  2,  2,  2,  2,  2, -2,  2,  2,\n",
       "       -2,  2,  2,  2,  2,  1,  1,  2,  2,  2,  0,  0,  0,  2, -2,  2,  2,\n",
       "        2,  2,  2,  1,  0, -2,  2,  2,  1,  2,  2,  1,  2,  1,  1,  1,  1,\n",
       "        2,  1,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  0,  0,  0, -2,  2,\n",
       "       -2,  2,  2,  0,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,\n",
       "        2,  0,  0,  2,  1,  2,  2, -2, -2,  1,  1,  2,  0,  0,  1,  1,  2,\n",
       "        2,  2, -2, -2, -2, -2, -1,  2, -1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2, -2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -2,\n",
       "       -2, -2,  2,  2,  2,  1,  1,  1,  2,  1,  1,  2,  2,  0,  0,  1,  1,\n",
       "        2,  1,  1,  2,  2,  2,  2,  2,  2,  1, -2,  1,  2,  2,  2,  2,  2,\n",
       "        1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  1,  1,  1,  2,  2,  2,  2,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,\n",
       "        0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  2,  1,  1,  2,  1,  1,  2,\n",
       "        2,  1,  1,  1,  1,  2,  1,  2,  1,  1,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  1,  1,  0,  0,  2,  2,  2,  2,  1,  1,  1,  2,  2,\n",
       "        2,  2,  2,  0,  2,  0,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1, -2,  2,  1,  2,  1,  2,  2,  2,  1,  1,  1,  1,  2,  1,  1,  1,\n",
       "        1,  1,  1,  0,  1,  1,  1,  1,  1,  0, -2, -2, -2, -2, -2, -2, -2,\n",
       "        0,  0,  0,  0,  0,  0,  1,  1,  1,  2,  1,  1,  2,  0, -2, -2,  1,\n",
       "        1,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,\n",
       "        1,  1,  0,  1,  0,  0,  0,  0,  0,  0, -1,  0, -2,  0, -2, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1, -2,  0,  0,  0,\n",
       "       -1,  0,  1, -1, -1, -1,  0,  0, -1,  0,  0,  0,  0,  0, -1,  0,  0,\n",
       "       -1, -1, -1, -1, -1, -1,  2,  1,  2,  2,  2,  2,  2,  2,  0,  0, -1,\n",
       "        0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,\n",
       "        0,  2,  2, -2,  2,  2,  2,  2,  2,  2,  2, -1,  0,  0,  2,  2,  2,\n",
       "        2,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,\n",
       "        2,  2,  2,  0,  1,  0,  1,  1,  1,  1,  1,  2, -1,  2, -1,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  2,  1, -1,  1,  1,  1,  1,  1,  0, -1, -1, -1, -1,  2, -1,\n",
       "       -1, -1,  1,  1,  2,  2,  2,  2, -1,  2,  2,  0, -1,  0,  1,  0,  0,\n",
       "        0,  0,  1,  0, -1, -1,  0,  0,  0,  0,  0,  0, -1,  1,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  2,  0,  2,  2,  2,\n",
       "        2,  0,  0, -2,  1,  0,  0,  0,  0,  1,  0, -2, -2, -2,  0,  0,  0,\n",
       "        0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, -2, -2,  0,  2,  2,\n",
       "        2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, -2,  0, -2,  2, -2,\n",
       "       -2, -2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
