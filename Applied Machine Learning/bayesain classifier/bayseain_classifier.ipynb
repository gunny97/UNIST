{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Build a simple naïve Bayes classifier to classify this dataset. You will use\\n20% of the data for testing and the other 80% for training. You should write\\nthis classifier yourself. Submit your Python code and your answers for the\\naccuracy of the classifier on the 20% test data, where accuracy is the\\nnumber of correct predictions as a fraction of total predictions. If you use\\nLaplace smoothing, mention it in your answers.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Build a simple naïve Bayes classifier to classify this dataset. You will use\n",
    "20% of the data for testing and the other 80% for training. You should write\n",
    "this classifier yourself. Submit your Python code and your answers for the\n",
    "accuracy of the classifier on the 20% test data, where accuracy is the\n",
    "number of correct predictions as a fraction of total predictions. If you use\n",
    "Laplace smoothing, mention it in your answers.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconditional probability: P(B|A) = P(A∩B)/P(B),P(A|B) = P(B∩A)/P(A)\\n\\nbayes theorem: P(A|B) = P(B|A) * P(A) / P(B)\\n-> P(A|B): posterior probability\\n-> P(B): prior probability of class\\n-> P(B|A): likelihood (probability of predictior given class)\\n-> P(A): prior probability of predictor\\n\\ngaussian density function: (1/√2pi*σ) * exp((-1/2)*((x-μ)²)/(2*σ²)),\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "conditional probability: P(B|A) = P(A∩B)/P(B),P(A|B) = P(B∩A)/P(A)\n",
    "\n",
    "bayes theorem: P(A|B) = P(B|A) * P(A) / P(B)\n",
    "-> P(A|B): posterior probability\n",
    "-> P(B): prior probability of class\n",
    "-> P(B|A): likelihood (probability of predictior given class)\n",
    "-> P(A): prior probability of predictor\n",
    "\n",
    "gaussian density function: (1/√2pi*σ) * exp((-1/2)*((x-μ)²)/(2*σ²)),\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \n",
    "    def mean_var_clac(self,feature,target):\n",
    "        '''\n",
    "        to find out each column's statistical values (mean, variance)\n",
    "        '''\n",
    "        self.mean = feature.groupby(target).apply(lambda x: np.mean(x))\n",
    "        self.mean = self.mean.to_numpy()\n",
    "        self.var = feature.groupby(target).apply(lambda x: np.var(x))\n",
    "        self.var = self.var.to_numpy()\n",
    "        return self.mean, self.var\n",
    "    \n",
    "    def gdf_prob(self,index_class,input_x):\n",
    "        '''\n",
    "        gaussian density function: (1/√2pi*σ) * exp((-1/2)*((x-μ)²)/(2*σ²))\n",
    "        to find out probability from gaussian density function\n",
    "        '''\n",
    "        mean = self.mean[index_class]\n",
    "        var = self.var[index_class]\n",
    "        return (np.exp((-1/2)*((input_x-mean)**2) / var)) / (np.sqrt(2 * np.pi * var))\n",
    "    \n",
    "    def prior_prob(self, feature, target):\n",
    "        '''\n",
    "        P(B): prior probability of class\n",
    "        to find out prior probability of class\n",
    "        '''\n",
    "        \n",
    "        def temp_func(x):\n",
    "            return len(x)/self.data_len\n",
    "        \n",
    "        self.prior_prob = feature.groupby(target).apply(temp_func)\n",
    "        self.prior_prob = self.prior_prob.to_numpy()\n",
    "                           \n",
    "        return self.prior_prob\n",
    "    \n",
    "    def post_prob(self, x):\n",
    "        '''\n",
    "        P(A|B): posterior probability\n",
    "        to find out posterior probability\n",
    "        '''\n",
    "        post_prob = []\n",
    "        for i in range(len(self.classes)):\n",
    "            # posterior = log(prior) + sum(log(likelihood))\n",
    "            post = np.log(self.prior_prob[i]) + np.sum(np.log(self.gdf_prob(i,x)))\n",
    "            post_prob.append(post)\n",
    "        return int(np.argmax(post_prob))\n",
    "    \n",
    "    def fit_(self,x_train, y_train):\n",
    "        '''\n",
    "        train data fitting on model\n",
    "        '''\n",
    "        self.classes = np.unique(y_train)\n",
    "        self.data_len = x_train.shape[0]\n",
    "        self.mean_var_clac(x_train,y_train)\n",
    "        self.prior_prob(x_train,y_train)\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        '''\n",
    "        test data predicting on model\n",
    "        '''\n",
    "        pred = []\n",
    "        for i in x_test.to_numpy():\n",
    "            pred.append(self.post_prob(i)) \n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self, y_test, y_pred):\n",
    "        '''\n",
    "        accuracy from predicted data with test data\n",
    "        '''\n",
    "        accuracy = np.sum(y_test == y_pred) / len(y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/김건우/Desktop/UNIST/4학년 1학기/기계학습 응용/pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = data[['6','148','72','35','0','33.6','0.627','50']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data, target, test_size=0.2, shuffle=True, stratify=target, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NaiveBayesClassifier()\n",
    "x.fit_(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = x.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8051948051948052"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "762    0\n",
       "763    0\n",
       "764    0\n",
       "765    1\n",
       "766    0\n",
       "Name: 1, Length: 767, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8051948051948052"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmy naive bayes classifier shows same score with sklearn.naive_bayes.\\nI do not use laplace smoothing method since I used Guassian Naive Bayes not Multinomal one.\\nI also do not preprocess the data before training the model.\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "my naive bayes classifier shows same score with sklearn.naive_bayes.\n",
    "I do not use laplace smoothing method since I used Guassian Naive Bayes not Multinomal one.\n",
    "I also do not preprocess the data before training the model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
